% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage[english]{babel}
\usepackage{xcolor}
\usepackage[nomain, toc, acronym]{glossaries}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
\renewcommand\UrlFont{\color{blue}\rmfamily}

\newacronym{cc}{CC}{Congestion Control}
\newacronym{cca}{CCA}{Congestion Control Algorithm}
\newacronym{aqm}{AQM}{Active Queue Management}
\newacronym{rtt}{RTT}{Round Trip Time}
\newacronym{fq}{FQ}{Fair Queuing}
\newacronym{bdp}{BDP}{Bandwidth Delay Product}
\newacronym{qoe}{QoE}{Quality of Experience}

\usepackage{amsmath,amssymb,amsfonts}
\newcommand{\mynote}[3]{
    \fbox{\bfseries\sffamily\scriptsize#1}
    {\small$\blacktriangleright$\textsf{\emph{\color{#3}{#2}}}$\blacktriangleleft$}}

\newcommand{\todo}[1]{\mynote{TODO}{#1}{red}}
\newcommand{\noteMax}[1]{\mynote{Max}{#1}{green}}

\def\anonymize{yes}
\def\tempYes{yes}
\def\tempNo{no}

\begin{document}
%
%JF: \title{Detecting Fair Queuing to Enable Better Congestion Control}
%JF: Alternative Vorschläge für Titel, etwas flüssiger:
\title{Optimizing Congestion Control Through Fair Queuing Detection}
%JF: \title{Optimum Congestion Control Selection Through Fair Queuing Detection}


%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\ifx\anonymize\tempYes
\author{Anonymous authors}
%
\authorrunning{Anonymous authors}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\else
\author{Maximilian Bachl\inst{1}\orcidID{0000-0001-9482-7428} \and
Joachim Fabini\inst{2,3}\orcidID{0000-0002-8285-1591} \and
Tanja Zseby\inst{3}\orcidID{0000-0002-5391-467X}}
%
\authorrunning{M. Bachl et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{TU Wien, Vienna, Austria; 
\email{firstname.lastname@tuwien.ac.at}}
\fi

\maketitle              % typeset the header of the contribution


%JF: FIXME: A fundamental problem that we should perhaps address in a brief statement: The proposed method does not work for cases in which load balancers are on the path. Distinct flows are likely to result in distinct hashes/flowIDs, meaning that the balancer will potentially assign them to distinct paths. So we may add a restrictive statement to the discussion that we assume that all flows share the same bottleneck? In addition we may state that no service/port priorization is in place.
%JF Any other idea?

\begin{abstract}
Low delay is an explicit requirement for applications such as cloud gaming and video conferencing. Delay-based congestion control can achieve the same throughput but significantly smaller delay than loss-based one and is thus ideal for these applications. However, when a delay- and a loss-based flow compete for a bottleneck, the loss-based one can monopolize all the bandwidth and starve the delay-based one. Fair queuing at the bottleneck link solves this problem by assigning an equal share of the available bandwidth to each flow. However, so far no end host based algorithm to detect fair queuing exists. Our contribution is the development of an algorithm that detects fair queuing at flow startup and chooses delay-based congestion control if there is fair queuing. Otherwise, loss-based congestion control can be used as a backup option. Results show that our algorithm reliably detects fair queuing and can achieve low delay and high throughput in case fair queuing is detected. 

\keywords{Congestion Control  \and Active Measurements \and Fair Queuing \and Queue Management.}
\end{abstract}
%
%
%
\section{Introduction}
\label{sec:introduction}

Emerging applications such as cloud gaming \cite{jarschel_evaluation_2011} and remote virtual reality \cite{elbamby_toward_2018} applications require high throughput and low delay. Furthermore, ultra low latency communications have been made a priority for 5G \cite{li_5g_2018}. We argue that for achieving high throughput as well as low delay, congestion control must be taken into account. 

\textit{Delay-based} \glspl{cca} were proposed to provide high throughput and low delay for network flows. They use the queuing delay as an indication of congestion and lower their throughput as soon as the delay increases. Such approaches have been proposed in the last century \cite{brakmo_tcp_1995} and recently have seen a surge in popularity \cite{arun_copa_2018,hock_tcp_2017,mittal_timely_2015,cardwell_bbr:_2016}. While delay-based \glspl{cca} fulfill their goal of low delay and high throughput, they cannot handle competing flows with different \glspl{cca} well \cite{turkovic_fifty_2019, turkovic_interactions_2019}. This is especially prevalent when they compete against loss-based \glspl{cca}, the latter being more ``aggressive''. While the delay-based \glspl{cca} back off as soon as queuing delay increases, the loss-based ones continue increasing their throughput until the buffer is full and packet loss occurs. This results in the loss-based flows monopolize the available bandwidth and starve the delay-based ones \cite{hock_toward_2016,yuan-cheng_lai_improving_2001,awdeh_comparing_2004}.

This unfairness can be mitigated by using \gls{fq} at the bottleneck link, isolating each flow from all other flows and assigning each flow an equal share of bandwidth \cite{dumazet_pkt_sched:_2013}. Consequently, loss-based flows can no longer acquire bandwidth that delay-based flows have released due to their network-friendly behavior. In this case, delay-based \glspl{cca} achieve their goal of high bandwidth and low delay. 

While \gls{fq} solves many problems regarding \gls{cc}, it is still not ubiquitously deployed. Flows can benefit from knowledge of \gls{fq} enabled bottleneck links on the path to dynamically adapt their \gls{cc}. In the case of \gls{fq} they can use a delay-based \gls{cca} and otherwise revert to a loss-based one. Our contribution is the design and evaluation of such a mechanism that determines the presence of \gls{fq} during the startup phase of a flow's \gls{cca} and sets the \gls{cca} accordingly at the end of the startup. 

We show that this solution reliably determines the presence of fair queuing and that by using this approach, queuing delay can be considerably lowered while maintaining throughput high if \gls{fq} is detected. In case no \gls{fq} is enabled, our algorithm detects this as well and a loss-based \gls{cc} is used, which does not starve when facing competition from other loss-based flows. 
%JF: delay-based was imo mistaken here!
While our mechanism is beneficial for network flows in general, we argue that it is most useful for long-running delay-sensitive flows, such as cloud gaming, video conferencing etc. 

To make our work reproducible and to encourage further research, we make the code, the results and the figures publicly available\footnote{\ifx\anonymize\tempYes $\langle$anonymized$\rangle$\else\url{https://github.com/CN-TU/PCC-Uspace}\fi}. 

\section{Related work}
Our work depends on active measurements for the startup phase of \gls{cc} and proposes a new measurement technique to detect \gls{aqm}. This section discusses preliminary work related to these fields. 

%JF: Evtl.statt subsubsection Absätze mit Titel fett verwenden um doch einigen Platz zu sparen?
\subsubsection{\gls{cc} using Active Measurements}
Several approaches have been recently proposed which aim to incorporate active measurements into \glspl{cca}. \cite{dong_pcc_2015,dong_pcc_2018} introduced the \gls{cca} \textit{PCC}, which uses active measurements to find the sending rate which optimizes a given reward function. \cite{cardwell_bbr:_2016} introduce the \textit{BBR} \gls{cca}, which uses active measurement to create a model of the network connection. This model is then used to determine the optimal sending rate. \cite{bachl_rax_2019} use Reinforcement Learning to learn \gls{cc} policies. This is done by performing active measurements by varying the congestion window to evaluate which value gives the highest throughput and lowest packet loss. 

%JF: Evtl.statt subsubsection Absätze mit Titel fett verwenden um doch einigen Platz zu sparen?
\subsubsection{Flow startup techniques} Besides the classical TCP slow start algorithm \cite{stevens_tcp_1997} some other proposals were made such as Quick Start \cite{jain_quick-start_2019}, which aims to speed up flow start by routers informing the end point, which rate they deem appropriate. \cite{jarvinen_congestion_2019} investigate the impact of \gls{aqm} and \gls{cc} during flow startup. They inspect a variety of different \gls{aqm} algorithms and also investigate \gls{fq}. \cite{kuhlewind_chirping_2010} propose ``chirping'' for flow startup to estimate the available bandwidth. This works by sending packets with different gaps between them to verify what the actual link speed is. \cite{mittal_recursively_2014} propose speeding up flow startup by using packets of different priority. The link is flooded with low-priority packets and in case the link is not fully utilized during startup, these low-priority packets are going to be transmitted, allowing the starting flow to achieve high throughput. If the link is already saturated, the low-priority packets are going to be discarded and no other flow suffers. 

%JF: Evtl.statt subsubsection Absätze mit Titel fett verwenden um doch einigen Platz zu sparen?
\subsubsection{Detecting \gls{aqm} mechanisms} There are few publications that investigate methods to detect the presence of an \gls{aqm}. \cite{kargar_bideh_tada_2016} propose a method to detect and distinguish certain popular \gls{aqm} mechanisms. However, unlike our work, it doesn't consider \gls{fq}. \cite{baykal_detection_2017} propose a machine learning based approach to fingerprint \gls{aqm} algorithm. \gls{fq} is not considered, too. 

\section{Concept}
\label{sec:concept}

The overall concept is the following:
\begin{enumerate}
\item During flow startup, determine whether the bottleneck link deploys \gls{fq} or not.
\item If \gls{fq} is used change to a delay-based \gls{cca} or revert to a loss-based \gls{cca} otherwise.
\end{enumerate}

\subsection{Algorithm to determine the presence of \gls{fq}}

%\begin{figure}
%\centering
%\includegraphics[width=1\columnwidth]{{"figures/fq_illustration_throughput"}.pdf}
%\caption{Sending rate}
%\label{fig:throughput}
%\end{figure}
%
%\begin{figure}
%\includegraphics[width=1\columnwidth]{{"figures/fq_illustration_goodput_no_fq"}.pdf}
%\caption{Receiving rate (goodput) in case the bottleneck link doesn't use \gls{fq}. Flow 2, which sends more also manages to get more data through the bottleneck.}
%\label{fig:goodput_no_fq}
%\end{figure}
%
%\begin{figure}
%\includegraphics[width=1\columnwidth]{{"figures/fq_illustration_goodput_fq"}.pdf}
%\caption{Receiving rate (goodput) in case the bottleneck link uses \gls{fq}. Flow 2, which sends more gets as much data through the bottleneck as flow 1.}
%\label{fig:goodput_fq}
%\end{figure}

\begin{figure}
\centering
\subfloat[Sending rate]{\includegraphics[width=0.33\columnwidth]{{"figures/fq_illustration_throughput"}.pdf}
\label{fig:throughput}}
\subfloat[Receiving rate (no \gls{fq}).]{\includegraphics[width=0.33\columnwidth]{{"figures/fq_illustration_goodput_no_fq"}.pdf}
\label{fig:goodput_no_fq}}
\subfloat[Receiving rate (\gls{fq}).]{\includegraphics[width=0.33\columnwidth]{{"figures/fq_illustration_goodput_fq"}.pdf}
\label{fig:goodput_fq}}
\caption{An example illustrating our proposed flow startup mechanism. Figure \ref{fig:throughput} shows the sending rate, \ref{fig:goodput_no_fq} the receiving rate in case there's no \gls{fq} and \ref{fig:goodput_fq} the receiving rate if there is \gls{fq}.}
\label{fig:illustration}
\end{figure}

The algorithm to determine the presence of \gls{fq} works as follows (seen from the sender's point of view):
\begin{enumerate}
\item Launch two concurrent flows, flow 1 and flow 2. Initialize flow 1 with some starting sending rate and flow 2 with two times that rate. 
\item After every \gls{rtt}, if no packet loss occurred, double the sending rate of both flows. 
\item If packet loss occurred in both flows in the previous \gls{rtt}, calculate the following metric: 
\begin{align}
\textit{loss ratio} &= \frac{\left(\frac{\textit{receiving rate of flow 1}}{\textit{sending rate of flow 1}}\right)}{\left(\frac{\textit{receiving rate of flow 2}}{\textit{sending rate of flow 2}}\right)}\end{align}

%JF: Perhaps abbreviate: sendRate flow1, recvRate flow1...
This loss ratio metric indicates if flow 2, which has a higher sending rate, achieves a higher receiving rate (goodput) than flow 1. If this is the case, there's no \gls{fq}, otherwise there is. 

It is necessary to wait for packet loss in both connections because only then it is certain that the link is saturated. Our algorithm assumes that the link is saturated. 
\item If \gls{fq} is used, the $\textit{loss ratio}$ is 2, otherwise it is 1. We choose 1.5 as the cutoff value. 
\item Since the presence or absence of \gls{fq} is now known the appropriate \gls{cca} can be launched. 
\end{enumerate}

Figure~\ref{fig:throughput}, \ref{fig:goodput_no_fq} and \ref{fig:goodput_fq} schematically illustrate this mechanism. We assume a sender being connected to a receiver, with a bottleneck link between them. On this bottleneck link, the queue is either managed by \gls{fq} or by a shared queue (no \gls{fq}). Figure~\ref{fig:throughput} shows how the sending rate (observed before the bottleneck link) is doubled after each \gls{rtt}. Furthermore, it shows that flow 2 has double the sending rate of flow 1. %\noteMax{Figures take up too much space. Is it better to put them next to each other? The actual problem is the nonsensical Springer format.}. 

Figure~\ref{fig:goodput_no_fq} shows the receiving rate at the receiver after the bottleneck link, if this bottleneck link has a a shared queue without \gls{fq}. In the third \gls{rtt} the sending rate exceeds the link speed of the bottleneck and thus the receiving rate is smaller than the sending rate. Because no \gls{fq} is employed, flow 2 manages to have a receiving rate two times the one of flow 1. 

Figure~\ref{fig:goodput_fq} shows the receiving rate of the two flows measured after the bottleneck link in case \gls{fq} is deployed. In the third \gls{rtt}, flow 1 and flow 2 achieve the same receiving rate because of the \gls{fq}, which makes sure that each flow gets an equal share of the total capacity. 

\subsection{Simple delay-based \gls{cc}}

If \gls{fq} is detected, a simple delay-based \gls{cc} is used from then on. Our algorithm is based on the two following simple rules:
\begin{enumerate}
\item If $\textit{current rtt} \leq \textit{smallest rtt ever measured on the connection} + 5\textit{ms}$ then increase the sending rate by 1\%.
\item Otherwise ($\textit{current rtt} > \textit{smallest rtt ever measured on the connection} + 5\textit{ms}$) decrease the sending rate by 5\%.
\end{enumerate}

This \gls{cca} is meant as a simple proof-of-concept to demonstrate the efficacy of our \gls{fq} detection mechanism but we do not claim it to be optimal in any way. 

\subsection{Fallback loss-based \gls{cc}}

If the absence of \gls{fq} is detected, a loss based \gls{cc} is used as a fallback. We use the algorithm \textit{PCC} proposed by \cite{dong_pcc_2015}. In simplified terms, this \gls{cca} aims to maximize a utility function, which strives for high throughput and low packet loss. However, it is reasonably aggressive and does not starve when competing with other loss-based \glspl{cca} such as Cubic \cite{ha_cubic_2008}.

\section{Implementation}

We base our implementation on the code of \textit{PCC Vivace} \cite{dong_pcc_2018}, which in turn is an improved version of PCC. The PCC code is based on the \textit{UDT} library \cite{gu_udt_2007}, which is a library that provides a reliable transport protocol similar to TCP on top of UDP. This approach is similar to QUIC \cite{iyengar_quic_2018}. An advantage of implementing our approach on top of UDP as opposed to TCP is that it is easier to develop and modify since no kernel module needs to be created and that it is more secure since the code is isolated in a user space process. 

\subsection{Deployment}

Our approach of detecting \gls{fq} relies on using two concurrent flows between the sender and the receiver. During flow startup, user data is transmitted in two flows that the receiver must detect and reassemble. This may be a challenge for deployments since solutions that require cooperation of several instances are more difficult to deploy at a wide scale in the Internet. %This can be seen for example when looking at the deployment of IPv6, which requires the participation of every router on the way and every end device. 
%JF: FIXME: I'm sceptical that the examples below work well in all scenarios. Distinct flows end up on distinct paths, so they do not necessarily use the same bottleneck.
However, multipath TCP \cite{handley_tcp_2012} and QUIC, which enable recombining data of several flows, are becoming increasingly prevalent and Google's QUIC implementation has multipath support \cite{google_quiche_2020}. We thus argue that the actual obstacles to deployment of our solution are constantly decreasing since QUIC (mostly Google traffic) already accounts for around %$\frac{1}{10}$ 
$10\%$ of all Internet traffic \cite{ruth_first_2018} and is growing. Moreover, Apple's macOS and iOS support multipath TCP \cite{apple_about_2018} support. 

\section{Results}

For the evaluation we use the network emulation library \texttt{py-virtnet}\footnote{\url{https://pypi.org/project/py-virtnet/}}. For the experiments with \gls{fq} we use the \texttt{fq} module for Linux, for the experiments with a shared queue we use \texttt{pfifo}. 

\subsection{Delay- vs.~loss-based \gls{cc}}

\begin{figure}[h]
\centering
\subfloat[Throughput (receiving rate)]{%
  \includegraphics[width=0.48\columnwidth]{{"figures/plots/throughput_1_just_one_flow_cubic_tcp_port9010_fq_10_10_20_1600183264284.pcap"}.pdf}%
  \label{fig:just_one_flow_tcp_throughput}
}\hfill
\subfloat[\gls{rtt}]{%
  \includegraphics[width=0.48\columnwidth]{{"figures/plots/rtt_1_just_one_flow_cubic_tcp_port9010_fq_10_10_20_1600183264284.pcap"}.pdf}%
  \label{fig:just_one_flow_tcp_delay}
}
\caption{Throughput and delay of a flow controlled by the loss-based \gls{cca} Cubic on a link with a speed of 10\,Mbps, a delay of 10\,ms and a buffer size of 100 packets (\texttt{pfifo}'s default value).}
\label{fig:just_one_flow_tcp}
\end{figure}

\begin{figure}[h]
\centering
\subfloat[Throughput (receiving rate)]{%
  \includegraphics[width=0.48\columnwidth]{{"figures/plots/throughput_1_just_one_flow_vegas_udp_port9000_fq_10_10_20_1600183241479.pcap"}.pdf}%
}\hfill
\subfloat[\gls{rtt}]{%
  \includegraphics[width=0.48\columnwidth]{{"figures/plots/rtt_1_just_one_flow_vegas_udp_port9000_fq_10_10_20_1600183241479.pcap"}.pdf}%
}
\caption{Throughput and delay of a flow controlled by our delay-based \gls{cca} on a link with a speed of 10\,Mbps, a delay of 10\,ms and a buffer size of 100 packets.}
\label{fig:just_one_flow_udp}
\end{figure}

First, we demonstrate empirically that delay-based \gls{cc} actually has a benefit over loss-based one. On one hand, \autoref{fig:just_one_flow_tcp} shows that the loss-based \gls{cc} achieves a throughput of almost 10\,Mbps on a 10\,Mbps link, which means full utilization and an \gls{rtt} that is  higher than 100\,ms. On the other hand, \autoref{fig:just_one_flow_udp} shows an example flow using our simple delay-based \gls{cca} and a slow start algorithm similar to the one used by the classic TCP \gls{cca} \textit{New Reno} \cite{allman_tcp_1999}. While the delay-based flow also achieves very high utilization, delay rarely exceeds 20\,ms, meaning that it is more than 5 times lower than the one of the loss-based \gls{cca} of \autoref{fig:just_one_flow_tcp}.

\subsection{Comparing flows with different Congestion Controls and AQM mechanisms}
In this scenario, we show an example of a flow with our mechanism that shares a bottleneck link with a flow using a loss-based congestion control. 
%JF: More compelling would be the use case when the delay-based flow is started first and the late-comer loss-based preempts it.

\subsubsection{No \gls{fq}}
The examples in \autoref{fig:pfifo_tcp} and in \autoref{fig:pfifo_udp} show the loss-based \gls{cca} Cubic compete against our delay-based \gls{cca} on a link without \gls{fq}. It is important to note that we do not use our \gls{fq} detection mechanism here but instead choose delay-based \gls{cc} from the beginning to demonstrate the starvation of delay-based flows that happens when they compete against loss-based ones without \gls{fq}. In the scenario, the loss-based flow starts 5 seconds earlier and after 5 seconds, the delay-based flow joins. While the delay-based flow gains some share of the link during startup, it is pushed away by the loss-based flow later on and ``starves''. We start the loss-based flow before to make sure that it has sufficient time to fill up the link. 

\begin{figure}[h]
\centering
\subfloat[Throughput (receiving rate)]{%
  \includegraphics[width=0.48\columnwidth]{{"figures/plots/throughput_1_competing_flow_pfifo_vegas_tcp_port9010_pfifo_10_50_20_1600182743408.pcap"}.pdf}%
}\hfill
\subfloat[\gls{rtt}]{%
  \includegraphics[width=0.48\columnwidth]{{"figures/plots/rtt_1_competing_flow_pfifo_vegas_tcp_port9010_pfifo_10_50_20_1600182743408.pcap"}.pdf}%
}
\caption{Throughput and delay of a flow controlled by the loss-based \gls{cca} Cubic on a link with a speed of 50\,Mbps, a delay of 10\,ms and a buffer size of 100 packets. The bottleneck is controlled by a shared queue (no \gls{fq}) and is shared with the flow of \autoref{fig:pfifo_udp}.}
\label{fig:pfifo_tcp}
\end{figure}

\begin{figure}[h]
\centering
\subfloat[Throughput (receiving rate)]{%
  \includegraphics[width=0.48\columnwidth]{{"figures/plots/throughput_1_competing_flow_pfifo_vegas_udp_port9000_pfifo_10_50_20_1600182743408.pcap"}.pdf}%
}\hfill
\subfloat[\gls{rtt}]{%
  \includegraphics[width=0.48\columnwidth]{{"figures/plots/rtt_1_competing_flow_pfifo_vegas_udp_port9000_pfifo_10_50_20_1600182743408.pcap"}.pdf}%
}
\caption{Throughput and delay of a flow controlled by our delay-based \gls{cca} on a link with a speed of 50\,Mbps, a delay of 10\,ms and a buffer size of 100 packets. The bottleneck is controlled by a shared queue (no \gls{fq}) and is shared with the flow of \autoref{fig:pfifo_tcp}.}
\label{fig:pfifo_udp}
\end{figure}

\subsubsection{\gls{fq}}

\begin{figure}[h]
\centering
\subfloat[Throughput (receiving rate)]{%
  \includegraphics[width=0.48\columnwidth]{{"figures/plots/throughput_1_competing_flow_fq_tcp_port9010_fq_10_50_20_1600182657352.pcap"}.pdf}%
}\hfill
\subfloat[\gls{rtt}]{%
  \includegraphics[width=0.48\columnwidth]{{"figures/plots/rtt_1_competing_flow_fq_tcp_port9010_fq_10_50_20_1600182657352.pcap"}.pdf}%
}
\caption{Throughput and delay of a flow controlled by the loss-based \gls{cca} Cubic on a link with a speed of 50\,Mbps, a delay of 10\,ms and a buffer size of 100 packets. The bottleneck is controlled by \gls{fq} and is shared with the flow of \autoref{fig:fq_udp}.}
\label{fig:fq_tcp}
\end{figure}

\begin{figure}[h]
\centering
\subfloat[Throughput (receiving rate)]{%
  \includegraphics[width=0.48\columnwidth]{{"figures/plots/throughput_1_competing_flow_fq_udp_port9000_fq_10_50_20_1600182657352.pcap"}.pdf}%
}\hfill
\subfloat[\gls{rtt}]{%
  \includegraphics[width=0.48\columnwidth]{{"figures/plots/rtt_1_competing_flow_fq_udp_port9000_fq_10_50_20_1600182657352.pcap"}.pdf}%
}
\caption{Throughput and delay of a flow controlled by our delay-based \gls{cca} on a link with a speed of 50\,Mbps, a delay of 10\,ms and a buffer size of 100 packets. The bottleneck is controlled by \gls{fq} and is shared with the flow of \autoref{fig:fq_tcp}.}
\label{fig:fq_udp}
\end{figure}

For a link with fair queuing, the following example results show that throughput is identical for our flow (\autoref{fig:fq_udp}) and the Cubic flow (\autoref{fig:fq_tcp}), just that the Cubic flow's delay is more than two times higher than the delay-based one's. Thus, our mechanism is effective to detect if the bottleneck uses fair queuing and use a delay-based congestion control if it is the case and a loss-based one otherwise. 

\subsection{Accuracy of the \gls{fq} detection mechanism}

To evaluate if our proposed mechanism correctly recognizes \gls{fq} in a systematic manner, we perform experiments using a wide range of network configurations: We vary bandwidth from 5 to 50\,Mbps, delay from 10 to 100\,ms and the buffer size from 1 to 100 packets, using a grid of these parameters. For each parameter, we take 5 values, evenly spaced. Thus, we run experiments with 125 different configurations. We run each configuration once with \gls{fq} and once without. Results show that for experiments without \gls{fq} (using a shared queue), in over 99\% of cases, the absence of \gls{fq} was correctly detected and in less than 1\% of cases, our algorithm detected \gls{fq} even though there was no \gls{fq}. For experiments, during which \gls{fq} was actually deployed on the bottleneck link, this was correctly detected in 97\% of cases and wrongly in 3\% of the time. The overall accuracy is thus 98\%, which we consider sufficient. 

To verify the behavior of our algorithm under the presence of cross traffic, we repeated the above experiments but with one bulk traffic flow using Cubic running at the same time, which we start 5 seconds before so that it has sufficient time to gain full bandwidth. Also under these circumstances we also achieve 98\% detection accuracy using our algorithm. 

\subsection{Systematic evaluation}
 
Besides the examples we provided, we also conducted a more systematic comparison of the throughput and delay that our algorithm achieves: We use the same scenario as above and again vary bandwidth from 5 to 50\,Mbps, delay from 10 to 100\,ms and the buffer size from 1 to 100 packets. For each parameter, we take 3 values, evenly spaced, totaling 27 distinct scenarios. We use \gls{fq} at the bottleneck link. We run each scenario both using Cubic as well as our algorithm for 20 seconds each. We average the achieved throughputs and average delays from all experiments for both Cubic as well as our algorithm. For Cubic, the mean throughput is 18\,Mbit/s and the mean \gls{rtt} is 109\,ms while for our algorithm it is 25\,Mbit/s and 74\,ms. This shows that our algorithm can correctly determine that there is \gls{fq} and switch to the delay-based \gls{cc} and that our algorithm achieves significantly lower delay. Surprisingly, our algorithm also achieves higher throughput, which is astonishing considering that loss-based \glspl{cca} such as Cubic should be more aggressive in filling the link. When analyzing the results we identified that Cubic performs very poorly in scenarios with a buffer of only 1 packet since it doesn't pace its packets and thus each time when two packets are sent right after another, packet loss occurs and Cubic lowers its rate. 

%\begin{table}[t]
%\caption{Comparison of for loss-based \glspl{cca} on a link of 100\;Mbit/s with an \gls{rtt} of 50\;ms. Utilization in \% of maximum capacity; \gls{rtt} in milliseconds. For Cubic fq achieves good performance. This is because its default buffer size of 100 packets coincidentially works well for this specific link speed and \gls{rtt}. We take the mean of 10 runs for each scenario.} \label{tab:performance_results}
%\centering
%\begin{tabular}{l r r r r r r} \toprule
%& \multicolumn{2}{l}{fq} & \multicolumn{2}{l}{fq\_codel} & \multicolumn{2}{l}{eggs} \\
%\cmidrule(r){2-3} \cmidrule(lr){4-5} \cmidrule(l){6-7}
%& Util. & \gls{rtt} & Util. & \gls{rtt} & Util. & \gls{rtt} \\ \midrule
%Cubic	& 96.2 & 54 & 92.6 & 52 & 98.5 & 66.5 \\
%Reno	& 84.1 & 52.7 & 81.1 & 51.5 & 97.9 & 98.2 \\
%\bottomrule
%\end{tabular}
%\end{table}

\section{Discussion}

During the implementation of our algorithm we noticed peculiar behavior when not using \gls{fq}: During startup, we make flow 1 have half the sending rate of flow 2. We would expect the receiving rates to be the same: flow 1 achieving around half the rate of flow 2. However, we noticed that usually flow 1 would have a receiving rate of 0. After investigating the issue we noticed that the reason was pacing: PCC (and other \glspl{cca}) such as BBR send packets in regular intervals, for example every millisecond, because sending regularly keeps the queues shorter. This behavior is called pacing. However, when there are two flows where one flow's sending rate is a multiple of the other's, an interleaving effect occurs: flow 1 would always send each packet slightly after flow 2 sends a packet. Then, when the packets arrive at the bottleneck, flow 2's packet, which arrives a bit earlier, fills up the buffer and flow 1's packet is consequently always dropped. As a solution, we do not send packets at regular intervals during startup but always add a small random value: For example, if the sending interval were 1\,ms, we would send the packet randomly in the interval from 0.5 to 1.5\,ms. This solves the aforementioned problem. The insight we gained from this is that while pacing is generally supportive, it is actually harmful if several flows compete at a bottleneck without \gls{fq} and one flow's sending rate is a multiple of the other's.

One problem of our proposed solution occurs when the bottleneck link changes while a flow is running. For example, the bottleneck link in the beginning could be the home router, while later is it becoming another router deeper in the core network. The problem occurs if the home router supports \gls{fq} and our algorithm detects this while the router in the core network, which becomes the bottleneck later on, doesn't support \gls{fq}. Then, our algorithm is going to use a delay-based \gls{cc} because that's what it determined to be correct in the beginning of the flow. However, later when the bottleneck changes, our algorithm is still using delay-based \gls{cc} even though the current bottleneck doesn't support \gls{fq} anymore. As a result, it might be possible that our algorithm uses the wrong \gls{cc}. One way to prevent this problem from occurring is to use periodic measurements. For example, our algorithm to determine \gls{fq} could be used every 5 or 10 seconds to measure again if the current bottleneck supports \gls{fq}. 

Another scenario that is worth noting is how our proposed solution deals with load balancing: In this paper we assume that both of our flows, which we use during startup to determine the presence of \gls{fq} take the same path to the host on the other side. However, with load balancing it could happen that flow 1 takes a different path than flow 2. Fortunately, several load balancing algorithms have been proposed whose aim is to make sure that connections, which use multiple paths using Multipath TCP or QUIC, stay on the same path \cite{olivier_bonaventure_multipath_2018,olteanu_datacenter_2016,lienardy_towards_2016}. Since we envision our solution to be based on Multipath TCP or QUIC load balancing should not be a problem. \textit{Apple Music}, for example, successfully load balances multipath connections \cite{olivier_bonaventure_apple_2019}. 

%JF: FIXME: Downside (if room left): it's not clear which element is the bottleneck link. If all of them are FQ or non-FW, then it's fine. Routing may result in other elements to be used that have not been probed. All routers along the path may need to provide periodic reports on queueing policy *and* on queue fill status, so the source can guess the bottleneck link.
%JF: And one interesting observation (perhaps follow-up work, but likely discussed already): I'm curious how a series of FQ and non-FQ routers impacts on competing flows. 
An alternative to our algorithm could be that all routers inform the endpoints if they support \gls{fq} or not. Such a solution could, for example, use an IP or a TCP extension, which every router uses to indicate if they're capable of \gls{fq}. Then, if every device on a path supports \gls{fq}, the client would use a delay-based \gls{cca}. However, while this solutions would achieve good results, the problem is that solutions, which require participation of every device in the Internet generally suffer from insufficient deployment because not every hardware vendor and network operator can be forced to implement the proposed solution for informing about \gls{fq} capability. 

Concluding, we argue that our new flow startup method can help to increase the deployment of delay-based \gls{cc}. Especially for applications that are very delay-sensitive, such as video conferencing and cloud gaming, the adoption of delay-based \gls{cc} can result in a significant increase of \gls{qoe}. We demonstrated that our method works using a prototype implementation using PCC but we are also confident that a similar flow startup method can also be integrated with other \glspl{cca} such as BBR. 

\bibliographystyle{splncs04}
\bibliography{fq_detection}

\end{document}
